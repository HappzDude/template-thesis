% !TeX spellcheck = en_US
 
\chapter{Technical Data}
\label{chap:ch2}
In section \ref{generalstack} to section \ref{alerting} we talk about how we grouped the responsibility of the tools into categories. In section \ref{monitoringenviroment} we describe the environment we tested the tools in. We also describe the technologies, which we used to build our testing environment

\section{General Stack}
\label{generalstack}
To explain a Stack in general, distinguish the difference between monitoring and logging. First, monitoring is running in the background and constantly collects the system data, these data is collected by specific metrics. However logging is only triggered by a definite event or exception.
With this knowledge it is a easier understanding how to establish a monitoring stack. Therefore 4 different types of tools are needed \cite{cau37427}.
First of all one for collect data from the system by specific metrics, the collector. The second type of tool is to store, maintain and querying them, the database. The third type of tool is to visualize the data, the visualization tool. Sometimes an alerting tool is also needed, but this is often integrated in the visualization tool.
Logging stacks have similar architecture, but in this case the Collector does not send metric data to the database. The log files of the whole system are collected in one place. These two types of APM can be done simultaneously.

\section{Collector}
To get data in a centralized spot a tool is needed to collect the data were it is generated and transport it to the server or provide an interface for the server to collect the data.
We call tools for this purpose collectors. There are collectors for every monitoring purpose. It is very common that a collector provides a general interface like XML or JSON files or can be adapted to variable databases to get a wide spectrum of cases to be applied.
 The monitored metrics dependent on the environment and the collector also has to use other tools that the system provides  to get this special type of metrics. In general the data collected can be split up in system data and application data. All system data are all physical values like CPU load, RAM and hard disc drive usage. These will be provided by cAdvisor (\ref{cadvisor}) in the case of Kubernetes. In the case of monitoring Kubernetes the number of jobs/pods or the number of connection per second/minute will be monitored. These and other data will be provided by the Api-server(\ref{apiserver}) of Kubernetes.
\subsection{cAdvisor}
\label{cadvisor}
Container Advisor (cAdvisor) is a tool for collecting, processing and exporting data of containers. It is originally designed for Docker but can be applied to any other container. All information about the container is accessible over a REST api that returns a JSON file that contains all data. A copy of cAdvisor is deployed within any Kubernetes Pod, so every APM tool can get the system metrics.
\subsection{Api-Server}
\label{apiserver} 
Api-Server is a tool that provides a RESTful interface and is a front end for the whole Kubernetes Cluster. Over the Api-Server a user is able to interact with all components of the cluster. 
\section{Database}
The databases for APM are usually time-series based (\ref{TS-Database}). As every other database it is used to make data persistent and perform request over multiple entities to get new informations about critical values and value changes over a time period.
Databases can offer two types of data providing methods. Most of the time the database provides a well defined interface which normally provides an authentication method to insert data into the database. Any snapshot then gets a time stamp.
The other method is that the database performs a get operation onto an interface provided by the collector. This type of data collection is better for static systems.

\subsection{Time-Series-Database}
\label{TS-Database}
 Time-Series-Database is a special kind of database developed for saving time related data. These data consist of arrays which are indexed by a time stamp. By the term Time-Series time ranges could also be used as primary key. These types of Databases are capable of creating, enumerating, updating, deleting and analyzing time-series-data.
Often they also allow you to merge multiple time-series together and make one data set out of them.
Like each other database, time-series-databases can also filter the data. They are also capable of ordering the dataset by values like time.

\section{Visualization} 
The Visualization tools are used to display the data stored in the databases in a comfortable and organized way. This is realized by plain text or graphs. Graphs have the big advantage to be able to display the value changes over time and can very easily illustrate spikes in the data sets. Furthermore graphs can present the data in more than one way, which makes it easier for people to detect abnormal data spikes.
Usually all this information can be accessed via a web interface as this also gives an option for identification. This is especially useful when the data are of high importance.
Often these tools also implement easy to use interfaces for alerting tools, to set conditions for specific alerts, which can save a lot of time.

\subsection{Graphs}
As previously mentioned, the data we collected from the cluster must be readout of the database and displayed in a readable fashion. Thus most visualization tools use graphs to display the collected data. 
Using graphs not only makes the data easy to read, but it also adds the option to scale the data to our needs and preferences. This can be very useful when looking for trends in a bigger time range.
It also gives the option of color coding the data, which can be useful to either see dangerous values more quickly, or simply render multiple data streams in one graph to compare them or to see them in comparison to the whole system.
\subsection{Permission Management}
Most visualization tools have a web interface in which all the data are displayed. To make sure only authorized people can view the data, these tools usually implement a few permission management methods. 
These can be ranging from simple login dialogs to viewing permissions of specific data streams only to authorized people. Some tools allow complete customization of the permission settings, while others offer a set of permission templates. The most popular method of authorization seems to be LDAP, as this can be used for simple and complex permission schemes alike. 
\subsubsection{LDAP}
\label{ldap}
Written-out Lightweight Directory Access Protocol is a Network-protocol on a client-server basis. LDAP describes the communication between the client and the LDAP Directory. The data-structure of LDAP is the so-called Directory Information Tree which is organized by one suffix(root) and nodes.  

\section{Alerting}
\label{alerting}
To inform the developer about the system, a tool is needed which is able to send warnings about predefined system states.
The alerting tool gets one or more error codes from the controller that is normally implemented into the database or visualization tool. Some tools combine these codes. The alerting tool then sends a warning or error message to all people involved. Most of the tools can send over multiple platforms. The most common are: E-Mail, SMS, Telegram and Slack. Many tools offer many of the mentioned interfaces and provide an api to integrate other alerting types.
Often the developers don’t want to get just one alert with one message, so some of the alerting tool have the possibility to group alerts. With this feature it is possible to group cascading events that are triggered by one failure. In some cases tools also supply an option to divide all alerts into critical alerts and warnings which can help the user to select the important messages.  

\section{Monitoring environment}
\label{monitoringenviroment}
As many monitoring tools are flexible to the environment we chose a specific setting for the tools to monitor cloud skills (Kubernetes). \cite{Vohra2016} 
\subsection{Docker}
Docker is an Open-Source software that virtualize Operation Systems (OS) with the concept of containers. That means that the application and the operating system is built in to one file and can be deployed out of that file with the before defined settings. This file is called an image. Docker also provides a collection of pre-built images on their page  \cite{Docker}. The software is as Kubernetes written in Go and under the Apache License.

\subsection{Kubernetes}
Kubernetes is an Open-Source system that provides a platform for container deployment and management. Its strength is in the field of scaling and maintaining the deployed containers. Kubernetes can run on different hosts at the same time. The Kubernetes master connects all hosts together to form one cluster out of them. The master also works as an interface for other systems to connect to the cluster. From the outside of the cluster it is not visible on which node the application is running. This is realized by a load balancer which is also running on the Kubernetes master. Every container in the cluster also runs a copy of cAdvisor (\ref{cadvisor}) which is very important for APM, because it collects data from the system like CPU and RAM and provides them over an interface.
\subsection{GO}
Go is a programming language that is as Kubernetes designed, implemented and updated by the Google Inc.. The basic idea about the language is to simplify the syntax compared to c and c++ by preserving their network capability and extend them in the field of cloud technologies. The language itself is capable of all modern concepts such as object orientation and typification and parallelization. It was developed as an Open-Source Software and first released in 2009.  

\subsection{Hardware}
We were Testing all the tools in the study on a 3 nodes Kubernetes cluster. Any cluster node has a 4 Core Intel CPU with 2.3 GHz and 4 Megabytes of Cache . The nodes also have 8 Gigabytes of Ram  each. The virtual machine is KVM which runs on a Fedora Linux.

\subsection{Sockshop}
The sockshop is a demo-application for micro services, which can be used to test monitoring microservices. It takes up quite a lot of resources, considering it is a website with a small collection of scripts behind it. This makes the sockshop a very good demo-application, as it produces enough data to test monitoring applications, or simply show the ability of a cluster to handle such tasks.
Sockshop features a slim but useful HTTP-based API which allows the system owner to retrieve or post data via its own scripts or micro services to complement the application or monitor data values from the inside.
\subsubsection{Architecture}
Sockshop is written in multiple languages. The Front-end is written in NodeJS to give the user a handy interface. A MySQL database is used to store all the items in the shop, which is especially useful when you have complex data sets for your products. To receive the data from the MySQL database an interface written in GO is used. 
GO is also used for the retrieval of the users, which are stored in a Mongo database, as here no complex data structures with a lot of connections are needed.
The cart of the shop uses Java to store its information inside a Mongo Database. 
The order process is done via a Java / .NET Core pipeline, which stores the orders inside a MongoDB. The stored orders are then processed by another Java pipeline to determine which orders can be shipped.

 
%Hier wird der Hauptteil stehen. Falls mehrere Kapitel gewünscht, entweder mehrmals \texttt{\textbackslash{}chapter} benutzen oder pro Kapitel eine eigene Datei anlegen und \texttt{ausarbeitung.tex} anpassen.

%LaTeX-Hinweise stehen in \cref{chap:latextipps}.

%noch etwas Fülltext
%\blinddocument
