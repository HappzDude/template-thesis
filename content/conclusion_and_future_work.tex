% !TeX spellcheck = en_US

\chapter{Conclusion}\label{chap:conclusion}
At the beginning of this paper we asked the question which monitoring tools are the best.
In \cref{result} we want to answer this question. In the \cref{generaltrends} an overview of the often use methods is given. At the end in \cref{futurework} an outlook on future work is presented. 
\section{Result}
\label{result}
The question on the best tools always depends on the environment that a tool is used in. Often a monitoring structure is already existing and new solutions have to be build around it.
To answer the question after testing we can recommend a monitoring stack for a Kubernetes cluster out of the tools we testes. In the test we had the best experience with an combination from Zabbix and Grafana . The installation is very easy and can be done with a single command. There are also different versions of the Zabbix implementation. Some of them are more light weighted or have more tools than the others. In terms of functions Zabbix provides nearly everything. To complete this functionality with usability and interoperability we recommend to configure a Grafana instance onto the REST API of Zabbix. This is easily done because Grafana has its own Zabbix plug-in. It provides a very nice dashboard view of the cluster metrics.
If there is any reason fur not using a Zabbix installation or if a deeper look into the single nodes and pods is required, we could recommend a combination of Heapster with InfluxDB and Grafana. The installation is slightly more complex because all the tools have to be configured to work together. The huge benefit is that all tools are adopted by the Kubernetes Developers to work great with the cluster. This not only allows a very detailed look on every pod that is deployed, but also ensures a good performance, even in small clusters. 

\section{General Trends}
\label{generaltrends}
Most of the systems we evaluated or considered evaluating were metric monitoring stacks. In general most of the tools do not detect the cause of the failure. Instead the effect is recognized and can be treated manually. Some tools left us with the impression of a trend towards more automation. These tools had functions like automated script triggering over SSH or automatic delivery of logs in the alert message. 
Another trend we saw is that alerting managers implement a massive amount of different services for messaging, like Slack or Telegram, that are used by modern developer teams. Some of the more prominent tools were also capable of REST or SOAP requests to give the option to implement a lot of different interfaces. To keep an overview over a cascading failure, some of the tools can group errors to one alert, thus minimizing the number of alerts.
In Visualization a big trend is the presentation of similar data in one single graph. These method provides a better overview over the system and leads to generalization of the data. Moreover most tools use coloring to highlight warnings or errors. A few tools were also capable of drag and drop graphs, enabling the user to setup a custom dashboard with their graphs.
We also saw some tendency in the database evolution. Databases are no longer just for storing single data points. They now have a greater added value by implementing there own querying language. Using this to purify the collected data for visualization tools. The most established data format is JSON because the files can be fragmented over multiple nodes.
The field of collectors drifts towards multi functional interfaces. This trend is started, because many systems have to be integrated in already existing monitoring environments. In addition there are two types of collectors. One for collecting logs and processing them and another for the metrics. Both are necessary to monitor a system as a whole.
When we installed and tested the tools, all of the above trends were reflected. That is how we identified and worked out the trends. 

\section{Future Work}
\label{futurework}
As a follow up to our work a deeper look into the field of log monitoring tools could provide a more efficient way of failure treatment. An important task is to look where microservice environments like Kubernetes or technology like docker store their logs. To get the best results from the logs, a good database with the capability of adapting to the quickly changing services is needed. We think a clear mapping of the logs to the services informations is a main objective of the work.
As we only looked at solutions that are open-source, there is the possibility to expand our work to the whole software market to take more tools into account. This work could discuss the question of benefits that open source has over paid tools for the new DevOps style of developing.

